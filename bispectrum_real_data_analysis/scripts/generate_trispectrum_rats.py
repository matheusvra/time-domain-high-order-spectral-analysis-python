import numpy as np
import pandas as pd
from high_order_spectra_analysis.time_domain_trispectrum.tdts import tdts
from pathos.multiprocessing import ProcessingPool as Pool
import os
from time import perf_counter
import pendulum
from bispectrum_real_data_analysis.scripts.utils import seconds_to_formatted_time
from loguru import logger
import mat73
from scipy.io import loadmat

BASE_PATH = os.getcwd() + "/bispectrum_real_data_analysis/scripts/rats_analysis/group5rat1"


class TDTS:
    def __init__(
        self,
        frequency_sampling: float,
        time: np.ndarray | None = None,
        frequency_array: np.ndarray | None = None,
        fmin: float | None = None,
        fmax: float | None = None,
        freq_step: float = 1e-3,
        phase_step: float = 1e-3,
        dtype: np.dtype = np.float64,
        enable_progress_bar: bool = True
    ):
        self.frequency_sampling = frequency_sampling
        self.time = time
        self.frequency_array = frequency_array
        self.fmin = fmin
        self.fmax = fmax
        self.freq_step = freq_step
        self.phase_step = phase_step
        self.dtype = dtype
        self.enable_progress_bar = enable_progress_bar

    def run_tbds(self, signal_dict: np.ndarray | pd.Series): 
        event, signal = list(signal_dict.items())[0]
        return {
            event: tdts(
                signal, 
                self.frequency_sampling, 
                time=self.time, 
                frequency_array=self.frequency_array, 
                fmin=self.fmin, 
                fmax=self.fmax, 
                freq_step=self.freq_step, 
                phase_step=self.phase_step, 
                dtype=self.dtype, 
                enable_progress_bar=self.enable_progress_bar
            )
        }


def load_data(rat_number: int, group_number: int, mat=False):
    """This method loads the data from the csv files generated by the script run_and_process_data.m.
    For each animal, the data must be generated, and then, the files must be copied to the data folder.
    Then, adjust the files names in the code below.

    Returns:
        data (pd.DataFrame): Dataframe with the data from the csv file.
        events_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_index_timestamp (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFP_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFPsec (pd.DataFrame): Dataframe with the data from the csv file.
        BASE_PATH (str): Path to the data folder.
    """


    BASE_PATH = "/home/matheus/Documents/repositories/bispectrum_real_data_analysis/bispectrum_real_data_analysis/data/rats"    

    logger.info(f"Loading {'.mat' if mat else '.csv'} files...")

    data_train_filename = f"{BASE_PATH}/G{group_number}-R{rat_number}_PreTreino{'_events.mat' if mat else '_to_bispectrum.csv'}"
    data_test_filename = f"{BASE_PATH}/G{group_number}-R{rat_number}_Salina{'_events.mat' if mat else '_to_bispectrum.csv'}"

    if mat:
        try:
            raw_data_train = mat73.loadmat(data_train_filename)
        except TypeError:
            raw_data_train = loadmat(data_train_filename)

        try:
            raw_data_test = mat73.loadmat(data_test_filename)
        except TypeError:
            raw_data_test = loadmat(data_test_filename)

        TimeSamplingTrain = 1.0/raw_data_train["srate"]
        TimeSamplingTest = 1.0/raw_data_test["srate"]

        timeTrain = np.arange(0, TimeSamplingTrain*raw_data_train["data"].shape[1] , TimeSamplingTrain)
        timeTest = np.arange(0, TimeSamplingTest*raw_data_test["data"].shape[1], TimeSamplingTest)

        data_train = pd.DataFrame(
            {
                "Time": timeTrain,
                "CS_modulating": raw_data_train["data"][0],
                "Inferior_colliculus": raw_data_train["data"][1]
            }
        ) 

        data_test = pd.DataFrame(
            {
                "Time": timeTest,
                "CS_modulating": raw_data_test["data"][0],
                "Inferior_colliculus": raw_data_test["data"][1]
            }
        ) 
    else:
        data_train = pd.read_csv(data_train_filename)
        data_test = pd.read_csv(data_test_filename)

    logger.success("Done!")

    return data_train, data_test    


def select_event_window(
    df: pd.DataFrame, 
    event_name: str, 
    samples_before: int = 0, 
    samples_after: int = 0
) -> pd.DataFrame:
    """
    Method to extract the slice of the dataframe which contais the event, with some data before and after, 
    given number of samples to add to the begin and end, respectively.
    """

    window_index = np.argwhere(df.event.to_numpy() == event_name).flatten()
    begin_index = window_index[0] - samples_before
    end_index = window_index[-1] + samples_after
    return df[begin_index:end_index]


if __name__ == "__main__":

    # Select the configuration to run the script
    # Warning: Depending on the configuration, the script may take a long time (hours) to run, be careful

    group_number: int = 7
    rat_number: int = 10

    for data_to_process in ("train", "test"):

        id_results: str = f"rato-{rat_number}-grupo-{group_number}-{data_to_process}"

        # Select the configuration to run the script, by commenting/uncommenting and changing the lines below

        # Configuration 1: Frequency array
        frequency_array = np.append(
            np.arange(start=4, stop=10, step=0.01), 
            np.arange(start=51.71, stop=55.71, step=0.01)
        )

        TDTS_PARAMETERS = {
            "frequency_array": frequency_array,
            "phase_step": 0.01
        }

        # From here, the script will run the same for all configurations
        data_train, data_test = load_data(rat_number=rat_number, group_number=group_number)

        data_dict = {
            "train": data_train,
            "test": data_test
        }

        data = data_dict[data_to_process]

        events = data.event.unique()
        events = events[events != "base"]
    
        logger.info(f"Events in the data: {events}")

        df_amps = pd.DataFrame()
        df_phases = pd.DataFrame()

        logger.info("Processing the tdts... This may take a while...\n")
        start_time = perf_counter()

        # Process the tdts for each channel, in parallel

        time = data.Time.to_numpy()
        TimeSampling = round(np.mean(time[1:] - time[:-1]), 6)
        FrequencySampling = 1.0/TimeSampling

        tdts_object = TDTS(
            frequency_sampling=FrequencySampling,
            time=None,
            frequency_array=TDTS_PARAMETERS.get("frequency_array"),
            fmin=TDTS_PARAMETERS.get("fmin"),
            fmax=TDTS_PARAMETERS.get("fmax"),
            freq_step=TDTS_PARAMETERS.get("freq_step"),
            phase_step=TDTS_PARAMETERS["phase_step"]
        )

        f = lambda x: tdts_object.run_tbds(x)

        with Pool() as pool:
            
            for result in pool.map(f, [{event: select_event_window(
                df=data,
                event_name=event,
                samples_before=0,
                samples_after=0
            ).loc[:, "Inferior_colliculus"].to_numpy()} for event in events]):
                
                event, result_data = list(result.items())[0]
                frequency_array, spectrum, phase_spectrum, bispectrum, phase_bispectrum, trispectrum, phase_trispectrum = result_data

                if "frequency" not in df_amps.columns:
                    df_amps = df_amps.assign(frequency=frequency_array)

                df_amps = df_amps.assign(**{f"tds_amp_{event}": spectrum})
                df_phases = df_phases.assign(**{f"tds_phase_{event}": phase_spectrum})

                df_amps = df_amps.assign(**{f"tdbs_amp_{event}": bispectrum})
                df_phases = df_phases.assign(**{f"tdbs_phase_{event}": phase_bispectrum})

                df_amps = df_amps.assign(**{f"tdts_amp_{event}": trispectrum})
                df_phases = df_phases.assign(**{f"tdts_phase_{event}": phase_trispectrum})


        hosa_df = pd.concat([df_amps, df_phases], axis=1)


        hosa_df.to_csv(f'{BASE_PATH}/hosa_{id_results}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)

        end_time = perf_counter()

        logger.success(f"Done processing {data_to_process} data. Elapsed time: {seconds_to_formatted_time(end_time - start_time)}")