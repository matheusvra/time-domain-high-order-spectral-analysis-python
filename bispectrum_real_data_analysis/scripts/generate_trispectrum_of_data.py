import numpy as np
import pandas as pd
from high_order_spectra_analysis.time_domain_trispectrum.tdts import tdts
from pathos.multiprocessing import ProcessingPool as Pool
import os
from time import perf_counter
import pendulum
from bispectrum_real_data_analysis.scripts.utils import seconds_to_formatted_time
from loguru import logger
from scipy import signal

class TDTS:
    def __init__(
        self,
        frequency_sampling: float, 
        time: np.ndarray | None = None,
        frequency_array: np.ndarray | None = None,
        fmin: float | None = None,
        fmax: float | None = None,
        freq_step: float = 1e-3,
        phase_step: float = 1e-3,
        dtype: np.dtype = np.float64,
        enable_progress_bar: bool = True
    ):
        self.frequency_sampling = frequency_sampling
        self.time = time
        self.frequency_array = frequency_array
        self.fmin = fmin
        self.fmax = fmax
        self.freq_step = freq_step
        self.phase_step = phase_step
        self.dtype = dtype
        self.enable_progress_bar = enable_progress_bar

    def run_tbds(self, signal_dict: np.ndarray | pd.Series) -> dict[str, tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]: 
        column, signal = list(signal_dict.items())[0]
        return {
            column: tdts(
                signal, 
                self.frequency_sampling, 
                time=self.time, 
                frequency_array=self.frequency_array, 
                fmin=self.fmin, 
                fmax=self.fmax, 
                freq_step=self.freq_step, 
                phase_step=self.phase_step, 
                dtype=self.dtype, 
                enable_progress_bar=self.enable_progress_bar
            )
        }


def load_data():
    """This method loads the data from the csv files generated by the script run_and_process_data.m.
    For each animal, the data must be generated, and then, the files must be copied to the data folder.
    Then, adjust the files names in the code below.

    Returns:
        data (pd.DataFrame): Dataframe with the data from the csv file.
        events_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_index_timestamp (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFP_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFPsec (pd.DataFrame): Dataframe with the data from the csv file.
        BASE_PATH (str): Path to the data folder.
    """
    BASE_PATH = os.getcwd() + "/bispectrum_real_data_analysis/data"
    data = pd.read_csv(f"{BASE_PATH}/data_matrix.csv", delimiter=',', encoding="utf8")
    events_index = pd.read_csv(f"{BASE_PATH}/events_index.csv", delimiter=',', encoding="utf8")
    events_index_data_array = np.full((len(data),), None)

    for start, end, event_idx in zip(events_index.start, events_index.end, np.arange(1, len(events_index))):
        events_index_data_array[start:end] = event_idx
        
    data = data.assign(events_index=events_index_data_array)
    events_index_timestamp = pd.read_csv(f"{BASE_PATH}/events_index_timestamp.csv", delimiter=',', encoding="utf8")
    events_behavior_TS_LFP_index = pd.read_csv(f"{BASE_PATH}/events_behavior_TS_LFPindex.csv", delimiter=',', encoding="utf8")

    # Inserting the events behavior data in the dataframe as a column

    events_behavior_TS_LFP_index_array = np.full((len(data),), None)

    for start, end, event_idx in zip(events_behavior_TS_LFP_index.start, events_behavior_TS_LFP_index.end, np.arange(1, len(events_behavior_TS_LFP_index))):
        events_behavior_TS_LFP_index_array[start:end] = event_idx
        
    data = data.assign(events_behavior_TS_LFP_index=events_behavior_TS_LFP_index_array)

    events_behavior_TS_LFPsec = pd.read_csv(f"{BASE_PATH}/events_behavior_TS_LFPsec.csv", delimiter=',', encoding="utf8")

    return data, events_index, events_index_timestamp, events_behavior_TS_LFP_index, events_behavior_TS_LFPsec, BASE_PATH

def select_event_window(
    df: pd.DataFrame, 
    event_number: int, 
    samples_before: int = 0, 
    samples_after: int = 0
) -> pd.DataFrame:
  """
  Method to extract the slice of the dataframe which contais the event, with some data before and after, 
  given number of samples to add to the begin and end, respectively.
  """
  
  window_index = np.argwhere(df.events_index.to_numpy() == event_number).flatten()
  begin_index = window_index[0] - samples_before
  end_index = window_index[-1] + samples_after
  return df[begin_index:end_index]


def decimate(data: pd.DataFrame, desired_frequency_sampling: float, filter: bool = False):
    time = data.Time.to_numpy()
    TimeSampling = round(np.mean(time[1:] - time[:-1]), 6)
    FrequencySampling = 1.0/TimeSampling
    logger.info(f"The time sampling is {TimeSampling} seconds and the frequency is "
        f"{FrequencySampling/float(1000**(FrequencySampling<=1000))} {'k'*bool(FrequencySampling>=1000)}Hz")

    newTimeSampling = 1.0/desired_frequency_sampling
    decimation_rate = np.ceil(newTimeSampling/TimeSampling).astype(int)
    logger.info(f"The data will be decimated by the rate 1:{decimation_rate}")

    if filter:
        matrix = data.iloc[:, 1:-2].to_numpy()
        decimated_matrix = signal.decimate(matrix, decimation_rate, axis=0, ftype='fir', zero_phase=True)
        new_data = data.copy()[::decimation_rate]
        new_data.iloc[:, 1:-2] = decimated_matrix
    else:
        new_data = data[::decimation_rate]

    TimeSampling = newTimeSampling
    
    FrequencySampling = 1.0/TimeSampling
    logger.info(f"The new time sampling is {np.round(TimeSampling, 5)} s and the new frequency is "
    f"{FrequencySampling/float(1000**(FrequencySampling>=1000))} {'k'*bool(FrequencySampling>=1000)}Hz")
    
    return new_data, TimeSampling, FrequencySampling


if __name__ == "__main__":

    # Select the configuration to run the script
    # Warning: Depending on the configuration, the script may take a long time (hours) to run, be careful

    id_results: str = "rato-001"

    # Select the configuration to run the script, by commenting/uncommenting and changing the lines below

    # Configuration 1: Frequency array
    frequency_array = np.arange(start=4, stop=10, step=0.1)
    frequency_array = np.append(frequency_array, np.arange(start=53.71 - 2, stop=53.71 + 2, step=0.01))

    # Configuration 2: Frequency range
    # TDBS_PARAMETERS = {
    #     "fmin": 0,
    #     "fmax": 60,
    #     "freq_step": 0.1,
    #     "phase_step": 0.01
    # }

    TDBS_PARAMETERS = {
        "frequency_array": frequency_array,
        "phase_step": 0.01
    }

    # From here, the script will run the same for all configurations
    full_data, events_index, events_index_timestamp, events_behavior_TS_LFP_index, events_behavior_TS_LFPsec, BASE_PATH = load_data()

    events = full_data.events_index.unique()
    events = events[events.astype(bool)]

    samples_before = 0
    samples_after = 0

    desired_frequency_sampling = 200

    data, TimeSampling, FrequencySampling = decimate(full_data, desired_frequency_sampling=desired_frequency_sampling, filter=True)

    for event_number in events:

        logger.info(f"\nProcessing event {event_number}")

        event_data = select_event_window(
            df=data,
            event_number=event_number,
            samples_before=samples_before,
            samples_after=samples_after
        )

        spectrum_df_amps = pd.DataFrame()
        spectrum_df_phases = pd.DataFrame()

        bispectrum_df_amps = pd.DataFrame()
        bispectrum_df_phases = pd.DataFrame()

        trispectrum_df_amps = pd.DataFrame()
        trispectrum_df_phases = pd.DataFrame()

        logger.info("Processing the tdts... This may take a while...\n")
        start_time = perf_counter()

        # Process the tdbs for each channel, in parallel

        tdbs_object = TDTS(
            frequency_sampling=FrequencySampling,
            time=None,
            frequency_array=TDBS_PARAMETERS.get("frequency_array"),
            fmin=TDBS_PARAMETERS.get("fmin"),
            fmax=TDBS_PARAMETERS.get("fmax"),
            freq_step=TDBS_PARAMETERS.get("freq_step"),
            phase_step=TDBS_PARAMETERS["phase_step"]
        )

        f = lambda x: tdbs_object.run_tbds(x)

        with Pool() as pool:
            channels_columns = event_data.columns[2:18]

            for result in pool.map(f, [{column: event_data[column].to_numpy()} for column in channels_columns]):
                pass
                column, result_data = list(result.items())[0]
                frequency_array, spectrum, phase_spectrum, bispectrum, phase_bispectrum, trispectrum, phase_trispectrum = result_data

                if "frequency" not in spectrum_df_amps.columns or "frequency" not in bispectrum_df_amps.columns:
                    spectrum_df_amps = spectrum_df_amps.assign(frequency=frequency_array)
                    bispectrum_df_amps = bispectrum_df_amps.assign(frequency=frequency_array)
                    trispectrum_df_amps = trispectrum_df_amps.assign(frequency=frequency_array)

                spectrum_df_amps = spectrum_df_amps.assign(**{f"tds_amp_{column}": spectrum})
                spectrum_df_phases = spectrum_df_phases.assign(**{f"tds_phase_{column}": phase_spectrum})

                bispectrum_df_amps = bispectrum_df_amps.assign(**{f"tdbs_amp_{column}": bispectrum})
                bispectrum_df_phases = bispectrum_df_phases.assign(**{f"tdbs_phase_{column}": phase_bispectrum})

                trispectrum_df_amps = trispectrum_df_amps.assign(**{f"tdts_amp_{column}": trispectrum})
                trispectrum_df_phases = trispectrum_df_phases.assign(**{f"tdts_phase_{column}": phase_trispectrum})


        spectrum_df = pd.concat([spectrum_df_amps, spectrum_df_phases], axis=1)
        bispectrum_df = pd.concat([bispectrum_df_amps, bispectrum_df_phases], axis=1)
        trispectrum_df = pd.concat([trispectrum_df_amps, trispectrum_df_phases], axis=1)

        spectrum_df.to_csv(f'{BASE_PATH}/spectrum_{id_results}-evento-{event_number}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)
        bispectrum_df.to_csv(f'{BASE_PATH}/bispectrum_{id_results}-evento-{event_number}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)
        trispectrum_df.to_csv(f'{BASE_PATH}/trispectrum_{id_results}-evento-{event_number}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)

        end_time = perf_counter()

        logger.success(f"Done. Elapsed time: {seconds_to_formatted_time(end_time - start_time)}")