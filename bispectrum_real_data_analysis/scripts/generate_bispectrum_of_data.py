import numpy as np
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import pandas as pd
from high_order_spectra_analysis.time_domain_bispectrum.tdbs import tdbs
from pathos.multiprocessing import ProcessingPool as Pool
import os
from time import perf_counter
import pendulum
from bispectrum_real_data_analysis.scripts.utils import seconds_to_formatted_time


def load_data():
    """This method loads the data from the csv files generated by the script run_and_process_data.m.
    For each animal, the data must be generated, and then, the files must be copied to the data folder.
    Then, adjust the files names in the code below.

    Returns:
        data (pd.DataFrame): Dataframe with the data from the csv file.
        events_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_index_timestamp (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFP_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFPsec (pd.DataFrame): Dataframe with the data from the csv file.
        BASE_PATH (str): Path to the data folder.
    """
    BASE_PATH = os.getcwd() + "/bispectrum_real_data_analysis/data"
    data = pd.read_csv(f"{BASE_PATH}/data_matrix.csv", delimiter=',', encoding="utf8")
    events_index = pd.read_csv(f"{BASE_PATH}/events_index.csv", delimiter=',', encoding="utf8")
    events_index_data_array = np.full((len(data),), None)

    for start, end, event_idx in zip(events_index.start, events_index.end, np.arange(1, len(events_index))):
        events_index_data_array[start:end] = event_idx
        
    data = data.assign(events_index=events_index_data_array)
    events_index_timestamp = pd.read_csv(f"{BASE_PATH}/events_index_timestamp.csv", delimiter=',', encoding="utf8")
    events_behavior_TS_LFP_index = pd.read_csv(f"{BASE_PATH}/events_behavior_TS_LFPindex.csv", delimiter=',', encoding="utf8")

    # Inserting the events behavior data in the dataframe as a column

    events_behavior_TS_LFP_index_array = np.full((len(data),), None)

    for start, end, event_idx in zip(events_behavior_TS_LFP_index.start, events_behavior_TS_LFP_index.end, np.arange(1, len(events_behavior_TS_LFP_index))):
        events_behavior_TS_LFP_index_array[start:end] = event_idx
        
    data = data.assign(events_behavior_TS_LFP_index=events_behavior_TS_LFP_index_array)

    events_behavior_TS_LFPsec = pd.read_csv(f"{BASE_PATH}/events_behavior_TS_LFPsec.csv", delimiter=',', encoding="utf8")

    return data, events_index, events_index_timestamp, events_behavior_TS_LFP_index, events_behavior_TS_LFPsec, BASE_PATH

def select_event_window(
    df: pd.DataFrame, 
    event_number: int, 
    samples_before: int = 0, 
    samples_after: int = 0
) -> pd.DataFrame:
  """
  Method to extract the slice of the dataframe which contais the event, with some data before and after, 
  given number of samples to add to the begin and end, respectively.
  """
  
  window_index = np.argwhere(df.events_index.to_numpy() == event_number).flatten()
  begin_index = window_index[0] - samples_before
  end_index = window_index[-1] + samples_after
  return df[begin_index:end_index]


def decimate(data, desired_frequency_sampling):
    backup_data = data.copy()
    time = backup_data.Time.to_numpy()
    TimeSampling = round(np.mean(time[1:] - time[:-1]), 6)
    FrequencySampling = 1.0/TimeSampling
    print(f"The time sampling is {TimeSampling} seconds and the frequency is "
        f"{FrequencySampling/float(1000**(FrequencySampling<=1000))} {'k'*bool(FrequencySampling>=1000)}Hz")

    newTimeSampling = 1.0/desired_frequency_sampling
    decimation_rate = np.ceil(newTimeSampling/TimeSampling).astype(int)
    print(f"The data will be decimated by the rate 1:{decimation_rate}")

    data = data[::decimation_rate]

    TimeSampling = newTimeSampling
    
    FrequencySampling = 1.0/TimeSampling
    print(f"The new time sampling is {np.round(TimeSampling, 5)} s and the new frequency is "
    f"{FrequencySampling/float(1000**(FrequencySampling>=1000))} {'k'*bool(FrequencySampling>=1000)}Hz")
    
    return data, TimeSampling, FrequencySampling, backup_data


def process_tdbs(
    column: list[str], 
    df: pd.DataFrame, 
    args_tdbs: dict
) -> dict:
    """ Method to process the tdbs for a given column

    Args:
        column (list[str]): column name
        df (pd.DataFrame): dataframe with the data
        args_tdbs (dict): arguments to the tdbs function

    Returns:
        dict: dict with the column name and the result of the tdbs
    """
    signal = df[column]

    if np.any([args_tdbs["fmin"], args_tdbs["fmax"], args_tdbs["freq_step"]]) and args_tdbs["frequency_array"] is None:
        raise ValueError("You must provide either the frequency array or the fmin, fmax and freq_step parameters")

    return {
        "column": column, 
        "result": tdbs(
            signal=signal,
            frequency_sampling=args_tdbs["frequency_sampling"],
            time=args_tdbs["time"],
            frequency_array=args_tdbs["frequency_array"],
            fmin=args_tdbs["fmin"],
            fmax=args_tdbs["fmax"],
            freq_step=args_tdbs["freq_step"],
            phase_step=args_tdbs["phase_step"],
            enable_progress_bar=False
        )
    }


if __name__ == "__main__":

    # Select the configuration to run the script
    # Warning: Depending on the configuration, the script may take a long time (hours) to run, be careful

    id_results: str = "rato-001"

    # Select the configuration to run the script, by commenting/uncommenting and changing the lines below

    # Configuration 1: Frequency array
    frequency_array = np.arange(start=4, stop=10, step=0.1)
    frequency_array = np.append(frequency_array, np.arange(start=53.71 - 2, stop=53.71 + 2, step=0.01))

    # Configuration 2: Frequency range
    # TDBS_PARAMETERS = {
    #     "fmin": 0,
    #     "fmax": 60,
    #     "freq_step": 0.1,
    #     "phase_step": 0.01
    # }

    TDBS_PARAMETERS = {
        "frequency_array": frequency_array,
        "phase_step": 0.01
    }

    # From here, the script will run the same for all configurations
    full_data, events_index, events_index_timestamp, events_behavior_TS_LFP_index, events_behavior_TS_LFPsec, BASE_PATH = load_data()

    events = full_data.events_index.unique()
    events = events[events.astype(bool)]

    samples_before = 0
    samples_after = 0
    for event_number in events:

        print(f"\nProcessing event {event_number}")

        event_data = select_event_window(
            df=full_data,
            event_number=event_number,
            samples_before=samples_before,
            samples_after=samples_after
        )

        desired_frequency_sampling = 200

        data, TimeSampling, FrequencySampling, backup_data = decimate(event_data, desired_frequency_sampling=desired_frequency_sampling)

        time = event_data.Time.to_numpy()

        spectrum_df_amps = pd.DataFrame()
        spectrum_df_phases = pd.DataFrame()

        bispectrum_df_amps = pd.DataFrame()
        bispectrum_df_phases = pd.DataFrame()

        print("Processing the tdbs... This may take a while...\n")
        start_time = perf_counter()

        # Process the tdbs for each channel, in parallel
        with Pool() as pool:
            channels_columns = event_data.columns[2:18]

            for result in pool.map(
                lambda column: process_tdbs(
                    column, 
                    event_data, 
                    {
                        "frequency_sampling": FrequencySampling,
                        "time": time,
                        "frequency_array": TDBS_PARAMETERS.get("frequency_array"),
                        "fmin": TDBS_PARAMETERS.get("fmin"),
                        "fmax": TDBS_PARAMETERS.get("fmax"),
                        "freq_step": TDBS_PARAMETERS.get("freq_step"),
                        "phase_step": TDBS_PARAMETERS["phase_step"]
                    }
                ), 
                channels_columns
            ):
                column = result["column"]
                signal = event_data[column].to_numpy()
                frequency_array, spectrum, phase_spectrum, bispectrum, phase_bispectrum = result["result"]

                if "frequency" not in spectrum_df_amps.columns or "frequency" not in bispectrum_df_amps.columns:
                    spectrum_df_amps = spectrum_df_amps.assign(frequency=frequency_array)
                    bispectrum_df_amps = bispectrum_df_amps.assign(frequency=frequency_array)

                spectrum_df_amps = spectrum_df_amps.assign(**{f"tds_amp_{column}": spectrum})
                spectrum_df_phases = spectrum_df_phases.assign(**{f"tds_phase_{column}": phase_spectrum})

                bispectrum_df_amps = bispectrum_df_amps.assign(**{f"tdbs_amp_{column}": bispectrum})
                bispectrum_df_phases = bispectrum_df_phases.assign(**{f"tdbs_phase_{column}": phase_bispectrum})


        spectrum_df = pd.concat([spectrum_df_amps, spectrum_df_phases], axis=1)
        bispectrum_df = pd.concat([bispectrum_df_amps, bispectrum_df_phases], axis=1)

        spectrum_df.to_csv(f'{BASE_PATH}/spectrum_{id_results}-evento-{event_number}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)
        bispectrum_df.to_csv(f'{BASE_PATH}/bispectrum_{id_results}-evento-{event_number}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)

        end_time = perf_counter()

        print(f"Done. Elapsed time: {seconds_to_formatted_time(end_time - start_time)}")

    