import numpy as np
import pandas as pd
from high_order_spectra_analysis.time_domain_bispectrum.tdbs import tdbs
from pathos.multiprocessing import ProcessingPool as Pool
import os
from time import perf_counter
import pendulum
from bispectrum_real_data_analysis.scripts.utils import seconds_to_formatted_time
from loguru import logger


BASE_PATH = os.getcwd() + "/bispectrum_real_data_analysis/data"


class TDBS:
    def __init__(
        self,
        frequency_sampling: float,
        time: np.ndarray | None = None,
        frequency_array: np.ndarray | None = None,
        fmin: float | None = None,
        fmax: float | None = None,
        freq_step: float = 1e-3,
        phase_step: float = 1e-3,
        dtype: np.dtype = np.float64,
        enable_progress_bar: bool = True
    ):
        self.frequency_sampling = frequency_sampling
        self.time = time
        self.frequency_array = frequency_array
        self.fmin = fmin
        self.fmax = fmax
        self.freq_step = freq_step
        self.phase_step = phase_step
        self.dtype = dtype
        self.enable_progress_bar = enable_progress_bar

    def run_tbds(self, signal_dict: np.ndarray | pd.Series): 
        column, signal = list(signal_dict.items())[0]
        return {
            column: tdbs(
                signal, 
                self.frequency_sampling, 
                time=self.time, 
                frequency_array=self.frequency_array, 
                fmin=self.fmin, 
                fmax=self.fmax, 
                freq_step=self.freq_step, 
                phase_step=self.phase_step, 
                dtype=self.dtype, 
                enable_progress_bar=self.enable_progress_bar
            )
        }


def load_data(rat_number: int):
    """This method loads the data from the csv files generated by the script run_and_process_data.m.
    For each animal, the data must be generated, and then, the files must be copied to the data folder.
    Then, adjust the files names in the code below.

    Returns:
        data (pd.DataFrame): Dataframe with the data from the csv file.
        events_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_index_timestamp (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFP_index (pd.DataFrame): Dataframe with the data from the csv file.
        events_behavior_TS_LFPsec (pd.DataFrame): Dataframe with the data from the csv file.
        BASE_PATH (str): Path to the data folder.
    """

    data_train_filename = f"R{rat_number}_PreTreino_to_bispectrum.csv"
    data_test_filename = f"R{rat_number}_Teste_to_bispectrum.csv"
    
    data_train = pd.read_csv(f'{BASE_PATH}/{data_train_filename}', delimiter=',', encoding="utf8")
    data_test = pd.read_csv(f'{BASE_PATH}/{data_test_filename}', delimiter=',', encoding="utf8")

    return data_train, data_test    


def select_event_window(
    df: pd.DataFrame, 
    event_name: str, 
    samples_before: int = 0, 
    samples_after: int = 0
) -> pd.DataFrame:
    """
    Method to extract the slice of the dataframe which contais the event, with some data before and after, 
    given number of samples to add to the begin and end, respectively.
    """

    window_index = np.argwhere(df.event.to_numpy() == event_name).flatten()
    begin_index = window_index[0] - samples_before
    end_index = window_index[-1] + samples_after
    return df[begin_index:end_index]


if __name__ == "__main__":

    # Select the configuration to run the script
    # Warning: Depending on the configuration, the script may take a long time (hours) to run, be careful

    rat_number: int = 1

    data_to_process = ("train", "test")[0]

    id_results: str = f"rato-{rat_number}-{data_to_process}"

    # Select the configuration to run the script, by commenting/uncommenting and changing the lines below

    # Configuration 1: Frequency array
    frequency_array = np.arange(start=51.71, stop=55.71, step=0.01)

    TDBS_PARAMETERS = {
        "frequency_array": frequency_array,
        "phase_step": 0.01
    }

    # From here, the script will run the same for all configurations
    data_train, data_test = load_data(rat_number=rat_number)

    data_dict = {
        "train": data_train,
        "test": data_test
    }

    data = data_dict[data_to_process]

    events = data.event.unique()
   
    logger.info(f"Events in the data: {events}")

    samples_before = 0
    samples_after = 0

    desired_frequency_sampling = 200

    for event in events:

        if event == "base":
            continue

        logger.info(f"\nProcessing event {event}")

        event_data = select_event_window(
            df=data,
            event_name=event,
            samples_before=samples_before,
            samples_after=samples_after
        )

        spectrum_df_amps = pd.DataFrame()
        spectrum_df_phases = pd.DataFrame()

        bispectrum_df_amps = pd.DataFrame()
        bispectrum_df_phases = pd.DataFrame()

        logger.info("Processing the tdbs... This may take a while...\n")
        start_time = perf_counter()

        # Process the tdbs for each channel, in parallel

        time = event_data.Time.to_numpy()
        TimeSampling = round(np.mean(time[1:] - time[:-1]), 6)
        FrequencySampling = 1.0/TimeSampling

        tdbs_object = TDBS(
            frequency_sampling=FrequencySampling,
            time=None,
            frequency_array=TDBS_PARAMETERS.get("frequency_array"),
            fmin=TDBS_PARAMETERS.get("fmin"),
            fmax=TDBS_PARAMETERS.get("fmax"),
            freq_step=TDBS_PARAMETERS.get("freq_step"),
            phase_step=TDBS_PARAMETERS["phase_step"]
        )

        f = lambda x: tdbs_object.run_tbds(x)

        with Pool() as pool:
            channels_columns = event_data.columns[2:3]

            for result in pool.map(f, [{column: event_data[column].to_numpy()} for column in channels_columns]):
                pass
                column, result_data = list(result.items())[0]
                frequency_array, spectrum, phase_spectrum, bispectrum, phase_bispectrum = result_data

                if "frequency" not in spectrum_df_amps.columns or "frequency" not in bispectrum_df_amps.columns:
                    spectrum_df_amps = spectrum_df_amps.assign(frequency=frequency_array)
                    bispectrum_df_amps = bispectrum_df_amps.assign(frequency=frequency_array)

                spectrum_df_amps = spectrum_df_amps.assign(**{f"tds_amp_{column}": spectrum})
                spectrum_df_phases = spectrum_df_phases.assign(**{f"tds_phase_{column}": phase_spectrum})

                bispectrum_df_amps = bispectrum_df_amps.assign(**{f"tdbs_amp_{column}": bispectrum})
                bispectrum_df_phases = bispectrum_df_phases.assign(**{f"tdbs_phase_{column}": phase_bispectrum})


        spectrum_df = pd.concat([spectrum_df_amps, spectrum_df_phases], axis=1)
        bispectrum_df = pd.concat([bispectrum_df_amps, bispectrum_df_phases], axis=1)

        spectrum_df.to_csv(f'{BASE_PATH}/spectrum_{id_results}-evento-{event}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)
        bispectrum_df.to_csv(f'{BASE_PATH}/bispectrum_{id_results}-evento-{event}_{"-".join(str(pendulum.today()).split("T")[0].split("-")[::-1])}.csv', index=False)

        end_time = perf_counter()

        logger.success(f"Done processint {data_to_process} data. Elapsed time: {seconds_to_formatted_time(end_time - start_time)}")